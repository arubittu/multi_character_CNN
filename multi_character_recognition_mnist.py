# -*- coding: utf-8 -*-
"""multi character recognition mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-m7btu65VuYRZEQ205xDvK38TKiugpS4
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

training_data=datasets.MNIST(root="data",
    train=True,
    download=True,
    transform=ToTensor()
)
test_data=datasets.MNIST(root="data",
    train=False,
    download=True,
    transform=ToTensor()
)
print(len(training_data))
print(len(test_data))

# visualizing
figure=plt.figure(figsize=(8,8))
for i in range(1,10):
  randim=torch.randint(len(training_data),size=(1,)).item()
  img,label=training_data[randim]
  img=torch.squeeze(img)
  figure.add_subplot(3,3,i)
  plt.title(label=label)
  plt.axis("off")
  plt.imshow(img,cmap='gray')
plt.show()
print(torch.unsqueeze(img,dim=0).size())

# load data in dataloader

from torch.utils.data import DataLoader

train_dataloader=DataLoader(dataset=training_data,batch_size=64,shuffle=True)
test_dataloader=DataLoader(dataset=test_data,batch_size=64,shuffle=True)

img_batch,label=next(iter(train_dataloader))
print(img_batch.size())
print(label.size())

import torch.autograd as autograd         # computation graph
from torch import Tensor                  # tensor node in the computation graph
import torch.nn as nn                     # neural networks
import torch.nn.functional as F           # layers, activations and more
import torch.optim as optim

class net(nn.Module):
  def __init__(self):
    super(net,self).__init__()
    self.convd1=nn.Conv2d(1,32,kernel_size=5,stride=1)
    self.max1=nn.MaxPool2d(kernel_size=2,stride=2)
    self.convd2=nn.Conv2d(32,64,kernel_size=5,stride=1)
    #self.convd3=nn.Conv2d(64,64,kernel_size=5,stride=1)
    self.max2=nn.MaxPool2d(kernel_size=2,stride=2)
    self.convd4=nn.Conv2d(64,10,kernel_size=4)
  
  def forward(self,x):
    x=F.relu(self.convd1(x))
    x=self.max1(x)
    x=F.relu(self.convd2(x))
    #x=F.relu(self.convd3(x))
    x=self.max2(x)
    x=self.convd4(x)
    return F.log_softmax(x, dim=1)
model=net()
model.to(device)

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
losss=nn.NLLLoss()
def train(epoch,model):      
  model.train()
  for batch_id,(data,label) in enumerate(train_dataloader):
    data,label=data.to(device),label.to(device)
    optimizer.zero_grad()
    output=model(data)
    
    # we use nll loss becuase we have added logsoftmax layer , or else we would have used cross entropy which combines both
    loss=losss(torch.squeeze(output),label)
    loss.backward()
    optimizer.step()
    
    if batch_id % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_id * len(data), len(train_dataloader.dataset),
                100. * batch_id / len(train_dataloader), loss.item()))
                                     
def test(model):
  model.eval()
  size = len(test_dataloader.dataset)
  pred_correct=0
  for batch_id,(data,label) in enumerate(test_dataloader):
    data,label=data.to(device),label.to(device)
    output=model(data)
    pred_correct += (label == torch.argmax(torch.squeeze(output),dim=1)).sum()
  
  print('the accuracy is {}'.format(pred_correct*100/size))

# lets train
for epoch in range(0,1):
  train(epoch,model)
  test(model)

figure=plt.figure(figsize=(12,12))
for i in range(1,10):
  randim=torch.randint(len(test_data),size=(1,)).item()
  img,label=test_data[randim]
  model.eval()
  pred_lb=torch.argmax(model(torch.unsqueeze(img.to(device),dim=0)),dim=1)

  img=torch.squeeze(img)
  figure.add_subplot(3,3,i)
  plt.title(label=pred_lb.item())
  plt.axis("off")
  plt.imshow(img,cmap='gray')
plt.show()
print(torch.unsqueeze(img,dim=0).size())

# multi char experiment
rand1=torch.randint(len(test_data),size=(1,)).item()
img1,label1=test_data[rand1]
rand2=torch.randint(len(test_data),size=(1,)).item()
img2,label2=test_data[rand2]
combined_image = torch.cat((img1,img2),dim=-1)
print('size of input img is {}'.format(combined_image.size()))

model.eval()
pred_lables=model(torch.unsqueeze(combined_image.to(device),dim=0))
print('shape of output predictions is {}'.format(pred_lables))

plt.imshow(torch.squeeze(combined_image),cmap='gray')
# as we can see, each of the ten tensors are for the respective classes, and for each class the 8 propablities are that of that class being there, hence 10,1 and 1,8 is high is log prob.

